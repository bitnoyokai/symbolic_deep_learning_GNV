{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QTA_EZGpYwlx"
      },
      "source": [
        "This notebook demonstrates our graph network inductive bias with a symbolic model extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lEKUuuYfTjP5"
      },
      "source": [
        "# Preamble and data generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IXpsg0LUb2sT"
      },
      "source": [
        "## Make sure to turn on the GPU via Edit-> Notebook settings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9imqRxveZl-J",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Importazione delle librerie di base ###\n",
        "\n",
        "# Basic pre-reqs:\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9RTQhSJDbfLZ"
      },
      "source": [
        "## Download pre-reqs, and then code for simulations and model files:\n",
        "\n",
        "(Note: installing torch-geometric may take a long time.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "colab_type": "code",
        "id": "7XHuxuzNg0Ko",
        "outputId": "844f96c1-520b-40a0-d6b7-8930ed7eecae",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install celluloid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "colab_type": "code",
        "id": "DQy02pANbx2e",
        "outputId": "c81fa951-cb3a-44f1-bda6-de617363c013",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "version_nums = torch.__version__.split('.')\n",
        "# Torch Geometric seems to always build for *.*.0 of torch :\n",
        "version_nums[-1] = '0' + version_nums[-1][1:]\n",
        "os.environ['TORCH'] = '.'.join(version_nums)\n",
        " \n",
        "!pip install --upgrade torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}.html && pip install --upgrade torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}.html && pip install --upgrade torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "colab_type": "code",
        "id": "MbF0-MSZbhUI",
        "outputId": "9737287f-6fb0-4a24-be86-45b1000ea86c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/MilesCranmer/symbolic_deep_learning/master/models.py -O models.py\n",
        "!wget https://raw.githubusercontent.com/MilesCranmer/symbolic_deep_learning/master/simulate.py -O simulate.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hVwSWJM-bl6Q",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Importazione dei codici per la modellizzazione della rete e della simulazione fisica: ###\n",
        "import models\n",
        "import simulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kdgdkJwFZl-V"
      },
      "source": [
        "## Assert we have a GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "colab_type": "code",
        "id": "krahINlfZl-W",
        "outputId": "33d6e7e1-8fb5-47ef-c2fc-fc84316ffaff",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "torch.ones(1).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zw9W7SB-Zl-i"
      },
      "source": [
        "## Create the simulation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "colab_type": "code",
        "id": "v8mC0I_lZl-j",
        "outputId": "dd430230-f1a9-4539-e612-467ad34f4e45",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Scelta dei parametri della simulazione: ###\n",
        "\n",
        "# Number of simulations to run (it's fast, don't worry):\n",
        "ns = 10000\n",
        "# Potential (see below for options)\n",
        "sim = 'spring'\n",
        "# Number of nodes\n",
        "n = 5\n",
        "# Dimension\n",
        "dim = 2\n",
        "# Number of time steps\n",
        "nt = 1000\n",
        "\n",
        "\n",
        "# Standard simulation sets:\n",
        "n_set = [4, 8]\n",
        "sim_sets = [\n",
        " {'sim': 'r1', 'dt': [5e-3], 'nt': [1000], 'n': n_set, 'dim': [2, 3]},\n",
        " {'sim': 'r2', 'dt': [1e-3], 'nt': [1000], 'n': n_set, 'dim': [2, 3]},\n",
        " {'sim': 'spring', 'dt': [1e-2], 'nt': [1000], 'n': n_set, 'dim': [2, 3]},\n",
        " {'sim': 'string', 'dt': [1e-2], 'nt': [1000], 'n': [30], 'dim': [2]},\n",
        " {'sim': 'charge', 'dt': [1e-3], 'nt': [1000], 'n': n_set, 'dim': [2, 3]},\n",
        " {'sim': 'superposition', 'dt': [1e-3], 'nt': [1000], 'n': n_set, 'dim': [2, 3]},\n",
        " {'sim': 'damped', 'dt': [2e-2], 'nt': [1000], 'n': n_set, 'dim': [2, 3]},\n",
        " {'sim': 'discontinuous', 'dt': [1e-2], 'nt': [1000], 'n': n_set, 'dim': [2, 3]},\n",
        "]\n",
        "\n",
        "\n",
        "# Select the hand-tuned dt value for a smooth simulation\n",
        "# (since scales are different in each potential):\n",
        "dt = [ss['dt'][0] for ss in sim_sets if ss['sim'] == sim][0]\n",
        "\n",
        "title = '{}_n={}_dim={}_nt={}_dt={}'.format(sim, n, dim, nt, dt)\n",
        "print('Running on', title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ARWJg6SbZl-n"
      },
      "source": [
        "## Generate simulation data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "38srsNZbZl-o",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from simulate import SimulationDataset\n",
        "s = SimulationDataset(sim, n=n, dim=dim, nt=nt//2, dt=dt)\n",
        "# Update this to your own dataset, or regenerate:\n",
        "base_str = './'\n",
        "data_str = title\n",
        "s.simulate(ns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "colab_type": "code",
        "id": "_iWH9BfUZl-r",
        "outputId": "4f70a812-4f4f-42bb-c066-23e92fb738c6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "data = s.data\n",
        "s.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8sCvhjgVpWIx"
      },
      "source": [
        "### Let's visualize an example simulation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "colab_type": "code",
        "id": "y5hFgzapZl-u",
        "outputId": "49a5e715-e36d-473c-c77d-e5ccafbac24b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Visualizzazione delle traittorie ###\n",
        "\n",
        "s.plot(0, animate=True, plot_size=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1PRxdcqSZl-z"
      },
      "source": [
        "## We'll train on the accelerations, so let's generate the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "j2j1M4coZl-3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Impostazione del dataset per l'allenamento ###\n",
        "\n",
        "accel_data = s.get_acceleration()\n",
        "\n",
        "X = torch.from_numpy(np.concatenate([s.data[:, i] for i in range(0, s.data.shape[1], 5)]))\n",
        "y = torch.from_numpy(np.concatenate([accel_data[:, i] for i in range(0, s.data.shape[1], 5)]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fz05x2HLT5Kg"
      },
      "source": [
        "# Set up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XWcn4EEFZl-_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.functional import F\n",
        "from torch.optim import Adam\n",
        "from torch_geometric.nn import MetaLayer, MessagePassing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E-uVcXRcZl_D",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from models import OGN, varOGN, make_packer, make_unpacker, get_edge_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kBtdfcvgZl_F"
      },
      "source": [
        "## Use the L1 regularization model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PyKcc5zbZl_H",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Scelta della Loss Function ###\n",
        "### Per ottenere bottleneck più accentuato, ridurre la dimensione del messaggio ### \n",
        "\n",
        "aggr = 'add'\n",
        "hidden = 300\n",
        "\n",
        "test = '_l1_'\n",
        "\n",
        "#This test applies an explicit bottleneck:\n",
        "\n",
        "msg_dim = 100\n",
        "n_f = data.shape[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oDiU99YHZl_K"
      },
      "source": [
        "L1 loss: we simply add the loss to the batch number. I.e., * 32 for batch size 32.\n",
        "\n",
        "KL loss: model the messages as a distribution with the prior a Gaussian. The means add in the final Gaussian. \n",
        "Recall in the D_KL(p||q), the prior is q.  Then, for sigma_q = 1, mu_q = 0, we have ($p=1$):\n",
        "\n",
        "$$D_{KL}(p||q) = \\frac{\\sigma_p^2 + \\mu_p^2}{2} -\\log({\\sigma_p}) - \\frac{1}{2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gTHRRgTuZl_K"
      },
      "source": [
        "## We use a custom data loader for the graphs for fast training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oL-1skODZl_L",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "from models import get_edge_index\n",
        "\n",
        "edge_index = get_edge_index(n, sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g46wRgIWZl_S"
      },
      "source": [
        "## Initiate the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1bs7l6xYZl_T",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "if test == '_kl_':\n",
        "    ogn = varOGN(n_f, msg_dim, dim, dt=0.1, hidden=hidden, edge_index=get_edge_index(n, sim), aggr=aggr).cuda()\n",
        "else:\n",
        "    ogn = OGN(n_f, msg_dim, dim, dt=0.1, hidden=hidden, edge_index=get_edge_index(n, sim), aggr=aggr).cuda()\n",
        "\n",
        "messages_over_time = []\n",
        "ogn = ogn.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i1SJ012hZl_V"
      },
      "source": [
        "### Let's test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "colab_type": "code",
        "id": "eSea_DJ-Zl_W",
        "outputId": "65989f6e-0ae4-4590-dd9d-a39291375619",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "_q = Data(\n",
        "    x=X_train[0].cuda(),\n",
        "    edge_index=edge_index.cuda(),\n",
        "    y=y_train[0].cuda())\n",
        "ogn(_q.x, _q.edge_index), ogn.just_derivative(_q).shape, _q.y.shape, ogn.loss(_q), "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RszfVgfhZl_a"
      },
      "source": [
        "# Set up training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WK5_u9F7UttT"
      },
      "source": [
        "## Organize into data loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8JyKnPWnZl_b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Organizzazion dei dati attraverso le funzioni importate da pytorch ###\n",
        "\n",
        "batch = int(64 * (4 / n)**2)\n",
        "trainloader = DataLoader(\n",
        "    [Data(\n",
        "        Variable(X_train[i]),\n",
        "        edge_index=edge_index,\n",
        "        y=Variable(y_train[i])) for i in range(len(y_train))],\n",
        "    batch_size=batch,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "testloader = DataLoader(\n",
        "    [Data(\n",
        "        X_test[i],\n",
        "        edge_index=edge_index,\n",
        "        y=y_test[i]) for i in range(len(y_test))],\n",
        "    batch_size=1024,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yyduWbndZl_e"
      },
      "source": [
        "## We'll use OneCycleLR for fast training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3dXp3rkuZl_f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hSqOrC1WZl_l"
      },
      "source": [
        "## Create the loss function\n",
        "\n",
        "This holds definition of our L1 and KL regularizations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U6EHSHiOZl_m",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Definizione delle funzioni di regolarizzazione ###\n",
        "\n",
        "def new_loss(self, g, augment=True, square=False):\n",
        "    if square:\n",
        "        return torch.sum((g.y - self.just_derivative(g, augment=augment))**2)\n",
        "    else:\n",
        "        base_loss = torch.sum(torch.abs(g.y - self.just_derivative(g, augment=augment)))\n",
        "        if test in ['_l1_', '_kl_']:\n",
        "            s1 = g.x[self.edge_index[0]]\n",
        "            s2 = g.x[self.edge_index[1]]\n",
        "            if test == '_l1_':\n",
        "                m12 = self.message(s1, s2)\n",
        "                regularization = 1e-2\n",
        "                #Want one loss value per row of g.y:\n",
        "                normalized_l05 = torch.sum(torch.abs(m12))\n",
        "                return base_loss, regularization * batch * normalized_l05 / n**2 * n\n",
        "            elif test == '_kl_':\n",
        "                regularization = 1\n",
        "                #Want one loss value per row of g.y:\n",
        "                tmp = torch.cat([s1, s2], dim=1)  # tmp has shape [E, 2 * in_channels]\n",
        "                raw_msg = self.msg_fnc(tmp)\n",
        "                mu = raw_msg[:, 0::2]\n",
        "                logvar = raw_msg[:, 1::2]\n",
        "                full_kl = torch.sum(torch.exp(logvar) + mu**2 - logvar)/2.0\n",
        "                return base_loss, regularization * batch * full_kl / n**2 * n\n",
        "        return base_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w12Qg4t_em8w"
      },
      "source": [
        "## Set up optimizer and training parameters:\n",
        "\n",
        "**Use 200 epochs for full version; can use fewer for test.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "colab_type": "code",
        "id": "y_KbxGDEZl_p",
        "outputId": "1c7f38d0-c33e-4c2b-ee92-480b36eed172",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Empiricamente per vari potenziali 150-200 epoche sembrano sufficienti per ottenere buoni risultati ###\n",
        "\n",
        "init_lr = 1e-3\n",
        "\n",
        "opt = torch.optim.Adam(ogn.parameters(), lr=init_lr, weight_decay=1e-8)\n",
        "\n",
        "total_epochs = 200\n",
        "\n",
        "batch_per_epoch = int(1000*10 / (batch/32.0))\n",
        "\n",
        "sched = OneCycleLR(opt, max_lr=init_lr,\n",
        "                   steps_per_epoch=batch_per_epoch,#len(trainloader),\n",
        "                   epochs=total_epochs, final_div_factor=1e5)\n",
        "\n",
        "batch_per_epoch\n",
        "\n",
        "epoch = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "s6Q_XHHOZl_v",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y4EzD79nU8DO"
      },
      "source": [
        "## Organize the recording of messages over time\n",
        "\n",
        "This is for fitting the forces, and extracting laws:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "J2jTb8r-Zl_z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as onp\n",
        "onp.random.seed(0)\n",
        "test_idxes = onp.random.randint(0, len(X_test), 1000)\n",
        "\n",
        "# Record messages over test dataset here:\n",
        "newtestloader = DataLoader(\n",
        "    [Data(\n",
        "        X_test[i],\n",
        "        edge_index=edge_index,\n",
        "        y=y_test[i]) for i in test_idxes],\n",
        "    batch_size=len(X_test),\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uxtqu4E4erXd"
      },
      "source": [
        "### Function to record messages from model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0k9hfsslZl_2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as onp\n",
        "import pandas as pd\n",
        "\n",
        "def get_messages(ogn):\n",
        "\n",
        "    def get_message_info(tmp):\n",
        "        ogn.cpu()\n",
        "\n",
        "        s1 = tmp.x[tmp.edge_index[0]]\n",
        "        s2 = tmp.x[tmp.edge_index[1]]\n",
        "        tmp = torch.cat([s1, s2], dim=1)  # tmp has shape [E, 2 * in_channels]\n",
        "        if test == '_kl_':\n",
        "            raw_msg = ogn.msg_fnc(tmp)\n",
        "            mu = raw_msg[:, 0::2]\n",
        "            logvar = raw_msg[:, 1::2]\n",
        "\n",
        "            m12 = mu\n",
        "        else:\n",
        "            m12 = ogn.msg_fnc(tmp)\n",
        "\n",
        "        all_messages = torch.cat((\n",
        "            s1,\n",
        "            s2,\n",
        "            m12), dim=1)\n",
        "        if dim == 2:\n",
        "            columns = [elem%(k) for k in range(1, 3) for elem in 'x%d y%d vx%d vy%d q%d m%d'.split(' ')]\n",
        "            columns += ['e%d'%(k,) for k in range(msg_dim)]\n",
        "        elif dim == 3:\n",
        "            columns = [elem%(k) for k in range(1, 3) for elem in 'x%d y%d z%d vx%d vy%d vz%d q%d m%d'.split(' ')]\n",
        "            columns += ['e%d'%(k,) for k in range(msg_dim)]\n",
        "\n",
        "\n",
        "        return pd.DataFrame(\n",
        "            data=all_messages.cpu().detach().numpy(),\n",
        "            columns=columns\n",
        "        )\n",
        "\n",
        "    msg_info = []\n",
        "    for i, g in enumerate(newtestloader):\n",
        "        msg_info.append(get_message_info(g))\n",
        "\n",
        "    msg_info = pd.concat(msg_info)\n",
        "    msg_info['dx'] = msg_info.x1 - msg_info.x2\n",
        "    msg_info['dy'] = msg_info.y1 - msg_info.y2\n",
        "    if dim == 2:\n",
        "        msg_info['r'] = np.sqrt(\n",
        "            (msg_info.dx)**2 + (msg_info.dy)**2\n",
        "        )\n",
        "    elif dim == 3:\n",
        "        msg_info['dz'] = msg_info.z1 - msg_info.z2\n",
        "        msg_info['r'] = np.sqrt(\n",
        "            (msg_info.dx)**2 + (msg_info.dy)**2 + (msg_info.dz)**2\n",
        "        )\n",
        "    \n",
        "    return msg_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RsLYGJX8Zl_5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "recorded_models = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lGJSTkWOZl_7"
      },
      "source": [
        "# Train the model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x-M9IDuiVrot"
      },
      "source": [
        "## Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "colab_type": "code",
        "id": "NHxCTt_tZl_7",
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "outputId": "b7f9e679-f490-4f39-92cb-61d7e6563666",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Qui avviene l'allenamento della rete ###\n",
        "\n",
        "for epoch in tqdm(range(epoch, total_epochs)):\n",
        "    ogn.cuda()\n",
        "    total_loss = 0.0\n",
        "    i = 0\n",
        "    num_items = 0\n",
        "    while i < batch_per_epoch:\n",
        "        for ginput in trainloader:\n",
        "            if i >= batch_per_epoch:\n",
        "                break\n",
        "            opt.zero_grad()\n",
        "            ginput.x = ginput.x.cuda()\n",
        "            ginput.y = ginput.y.cuda()\n",
        "            ginput.edge_index = ginput.edge_index.cuda()\n",
        "            ginput.batch = ginput.batch.cuda()\n",
        "            if test in ['_l1_', '_kl_']:\n",
        "                loss, reg = new_loss(ogn, ginput, square=False)\n",
        "                ((loss + reg)/int(ginput.batch[-1]+1)).backward()\n",
        "            else:\n",
        "                loss = ogn.loss(ginput, square=False)\n",
        "                (loss/int(ginput.batch[-1]+1)).backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            i += 1\n",
        "            num_items += int(ginput.batch[-1]+1)\n",
        "\n",
        "    cur_loss = total_loss/num_items\n",
        "    print(cur_loss)\n",
        "    cur_msgs = get_messages(ogn)\n",
        "    cur_msgs['epoch'] = epoch\n",
        "    cur_msgs['loss'] = cur_loss\n",
        "    messages_over_time.append(cur_msgs)\n",
        "    \n",
        "    ogn.cpu()\n",
        "    from copy import deepcopy as copy\n",
        "    recorded_models.append(ogn.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YAqyXbr9dpKE"
      },
      "source": [
        "## Save and Load models here to prevent re-training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7eQ9087RZmAA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Salvataggio di tutte i messaggi ottenuti dalla simulazione e predetti dalla rete ###\n",
        "\n",
        "import pickle as pkl\n",
        "pkl.dump(messages_over_time,\n",
        "    open('messages_over_time.pkl', 'wb'))\n",
        "messages_over_time = pkl.load(open('messages_over_time.pkl', 'rb'))\n",
        "\n",
        "pkl.dump(recorded_models,\n",
        "    open('models_over_time.pkl', 'wb'))\n",
        "\n",
        "recorded_models = pkl.load(open('models_over_time.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iC7pYpZnVwFG"
      },
      "source": [
        "# Analyze trained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c3R-CJtQZmAJ"
      },
      "source": [
        "## Plot a comparison of the force components with the messages:\n",
        "\n",
        "### (Or plot the rotation or sparsity - turn these on and off with flags:)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "x4slGzWtZmAS",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "d82a2e68-5f63-4673-8748-9042974c134f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Analisi grafica dei risultati: ###\n",
        "### * Messaggi vs Forze ###\n",
        "### * Messaggi nel tempo in scala di grigi ###\n",
        "### * Rappresentazione rotazionale dei messaggi ### \n",
        "\n",
        "from celluloid import Camera\n",
        "from copy import deepcopy as copy\n",
        "\n",
        "# Only turn on one of these:\n",
        "plot_force_components = True\n",
        "plot_sparsity = False\n",
        "plot_rotation = False\n",
        "if plot_force_components:\n",
        "    fig, ax = plt.subplots(1, dim, figsize=(4*dim, 4))\n",
        "if plot_sparsity or plot_rotation:\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "cam = Camera(fig)\n",
        "\n",
        "\n",
        "last_alpha_x1 = 0.0\n",
        "last_alpha_y1 = 0.0\n",
        "t = lambda _: _#tqdm\n",
        "for i in t(range(0, len(messages_over_time), 1)):\n",
        "    msgs = copy(messages_over_time[i])\n",
        "\n",
        "    msgs['bd'] = msgs.r + 1e-2\n",
        "\n",
        "    try:\n",
        "        msg_columns = ['e%d'%(k) for k in range(1, msg_dim+1)]\n",
        "        msg_array = np.array(msgs[msg_columns])\n",
        "    except:\n",
        "        msg_columns = ['e%d'%(k) for k in range(msg_dim)]\n",
        "        msg_array = np.array(msgs[msg_columns])\n",
        "\n",
        "    msg_importance = msg_array.std(axis=0)\n",
        "    most_important = np.argsort(msg_importance)[-dim:]\n",
        "    msgs_to_compare = msg_array[:, most_important]\n",
        "    msgs_to_compare = (msgs_to_compare - np.average(msgs_to_compare, axis=0)) / np.std(msgs_to_compare, axis=0)\n",
        "\n",
        "    if plot_sparsity:\n",
        "        ax.pcolormesh(msg_importance[np.argsort(msg_importance)[::-1][None, :15]], cmap='gray_r', edgecolors='k')\n",
        "        # plt.colorbar()\n",
        "        plt.axis('off')\n",
        "        plt.grid(True)\n",
        "        ax.set_aspect('equal')\n",
        "        plt.text(15.5, 0.5, '...', fontsize=30)\n",
        "        # fig.suptitle(title + test + 'mse=%.3e'%(min_result.fun/len(msgs),))\n",
        "        plt.tight_layout()\n",
        "    \n",
        "    if plot_force_components or plot_rotation:\n",
        "        pos_cols = ['dx', 'dy']\n",
        "        if dim == 3:\n",
        "            pos_cols.append('dz')\n",
        "\n",
        "        if sim != 'spring':\n",
        "            raise NotImplementedError(\"The current force function is for a spring. You will need to change the force function below to that expected by your simulation.\")\n",
        "        force_fnc = lambda msg: -(msg.bd - 1)[:, None] * np.array(msg[pos_cols]) / msg.bd[:, None]\n",
        "\n",
        "        expected_forces = force_fnc(msgs)\n",
        "\n",
        "        def percentile_sum(x):\n",
        "            x = x.ravel()\n",
        "            bot = x.min()\n",
        "            top = np.percentile(x, 90)\n",
        "            msk = (x>=bot) & (x<=top)\n",
        "            frac_good = (msk).sum()/len(x)\n",
        "            return x[msk].sum()/frac_good\n",
        "\n",
        "        from scipy.optimize import minimize\n",
        "\n",
        "        def linear_transformation_2d(alpha):\n",
        "\n",
        "            lincomb1 = (alpha[0] * expected_forces[:, 0] + alpha[1] * expected_forces[:, 1]) + alpha[2]\n",
        "            lincomb2 = (alpha[3] * expected_forces[:, 0] + alpha[4] * expected_forces[:, 1]) + alpha[5]\n",
        "\n",
        "            score = (\n",
        "                percentile_sum(np.square(msgs_to_compare[:, 0] - lincomb1)) +\n",
        "                percentile_sum(np.square(msgs_to_compare[:, 1] - lincomb2))\n",
        "            )/2.0\n",
        "\n",
        "            return score\n",
        "\n",
        "        def out_linear_transformation_2d(alpha):\n",
        "            lincomb1 = (alpha[0] * expected_forces[:, 0] + alpha[1] * expected_forces[:, 1]) + alpha[2]\n",
        "            lincomb2 = (alpha[3] * expected_forces[:, 0] + alpha[4] * expected_forces[:, 1]) + alpha[5]\n",
        "\n",
        "            return lincomb1, lincomb2\n",
        "\n",
        "        def linear_transformation_3d(alpha):\n",
        "\n",
        "            lincomb1 = (alpha[0] * expected_forces[:, 0] + alpha[1] * expected_forces[:, 1] + alpha[2] * expected_forces[:, 2]) + alpha[3]\n",
        "            lincomb2 = (alpha[0+4] * expected_forces[:, 0] + alpha[1+4] * expected_forces[:, 1] + alpha[2+4] * expected_forces[:, 2]) + alpha[3+4]\n",
        "            lincomb3 = (alpha[0+8] * expected_forces[:, 0] + alpha[1+8] * expected_forces[:, 1] + alpha[2+8] * expected_forces[:, 2]) + alpha[3+8]\n",
        "\n",
        "            score = (\n",
        "                percentile_sum(np.square(msgs_to_compare[:, 0] - lincomb1)) +\n",
        "                percentile_sum(np.square(msgs_to_compare[:, 1] - lincomb2)) +\n",
        "                percentile_sum(np.square(msgs_to_compare[:, 2] - lincomb3))\n",
        "            )/3.0\n",
        "\n",
        "            return score\n",
        "\n",
        "        def out_linear_transformation_3d(alpha):\n",
        "\n",
        "            lincomb1 = (alpha[0] * expected_forces[:, 0] + alpha[1] * expected_forces[:, 1] + alpha[2] * expected_forces[:, 2]) + alpha[3]\n",
        "            lincomb2 = (alpha[0+4] * expected_forces[:, 0] + alpha[1+4] * expected_forces[:, 1] + alpha[2+4] * expected_forces[:, 2]) + alpha[3+4]\n",
        "            lincomb3 = (alpha[0+8] * expected_forces[:, 0] + alpha[1+8] * expected_forces[:, 1] + alpha[2+8] * expected_forces[:, 2]) + alpha[3+8]\n",
        "\n",
        "            return lincomb1, lincomb2, lincomb3\n",
        "\n",
        "        if dim == 2:\n",
        "            min_result = minimize(linear_transformation_2d, np.ones(dim**2 + dim), method='Powell')\n",
        "        if dim == 3:\n",
        "            min_result = minimize(linear_transformation_3d, np.ones(dim**2 + dim), method='Powell')\n",
        "        print(title, test, 'gets', min_result.fun/len(msgs))\n",
        "\n",
        "        if plot_rotation:\n",
        "            q = min_result.x\n",
        "            alphax1, alphay1, offset1 = q[:3]\n",
        "            alphax2, alphay2, offset2 = q[3:]\n",
        "            \n",
        "            s1 = alphax1**2 + alphay1**2\n",
        "            s2 = alphax2**2 + alphay2**2\n",
        "            \n",
        "            if (\n",
        "                    (alphax2 - last_alpha_x1)**2\n",
        "                    + (alphay2 - last_alpha_y1)**2  <\n",
        "                   (alphax1 - last_alpha_x1)**2\n",
        "                    + (alphay1 - last_alpha_y1)**2):\n",
        "                \n",
        "                alphax1, alphay1, offset1 = q[3:]\n",
        "                alphax2, alphay2, offset2 = q[:3]\n",
        "                \n",
        "            last_alpha_x1 = alphax1\n",
        "            last_alpha_y1 = alphay1\n",
        "            s1 = alphax1**2 + alphay1**2\n",
        "            s2 = alphax2**2 + alphay2**2\n",
        "            alphax1 /= s1**0.5 * 2\n",
        "            alphay1 /= s1**0.5 * 2\n",
        "            alphax2 /= s2**0.5 * 2\n",
        "            alphay2 /= s2**0.5 * 2\n",
        "            \n",
        "            ax.arrow(0.5, 0.5, alphax1, alphay1, color='k', head_width=0.05, length_includes_head=True)\n",
        "            ax.arrow(0.5, 0.5, alphax2, alphay2, color='k', head_width=0.05, length_includes_head=True)\n",
        "            ax.axis('off')\n",
        "        \n",
        "        if plot_force_components:\n",
        "            for i in range(dim):\n",
        "                if dim == 3:\n",
        "                    px = out_linear_transformation_3d(min_result.x)[i]\n",
        "                else:\n",
        "                    px = out_linear_transformation_2d(min_result.x)[i]\n",
        "\n",
        "                py = msgs_to_compare[:, i]\n",
        "                ax[i].scatter(px, py,\n",
        "                              alpha=0.1, s=0.1, color='k')\n",
        "                ax[i].set_xlabel('Linear combination of forces')\n",
        "                ax[i].set_ylabel('Message Element %d'%(i+1))\n",
        "\n",
        "                xlim = np.array([np.percentile(px, q) for q in [10, 90]])\n",
        "                ylim = np.array([np.percentile(py, q) for q in [10, 90]])\n",
        "                xlim[0], xlim[1] = xlim[0] - (xlim[1] - xlim[0])*0.05, xlim[1] + (xlim[1] - xlim[0])*0.05\n",
        "                ylim[0], ylim[1] = ylim[0] - (ylim[1] - ylim[0])*0.05, ylim[1] + (ylim[1] - ylim[0])*0.05\n",
        "\n",
        "                ax[i].set_xlim(xlim)\n",
        "                ax[i].set_ylim(ylim)\n",
        "                \n",
        "        plt.tight_layout()\n",
        "    \n",
        "    cam.snap()\n",
        "\n",
        "ani = cam.animate()\n",
        "    \n",
        "from IPython.display import HTML\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "huBwFGpGZmAa"
      },
      "source": [
        "## Plot some predicted versus true trajectories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "colab_type": "code",
        "id": "-9fKkNNeZmAi",
        "outputId": "d1cbb00f-484f-4896-9c34-84039ce3df61",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Confronto grafico delle traiettorie predette con quelle simulate: ###\n",
        "### Nel grafico di destra: ###\n",
        "### * i cerchi colorati sono le tratiettorie predette; ###\n",
        "### * i cerchi grigi sono le traiettorie simulate. ###\n",
        "\n",
        "from simulate import make_transparent_color\n",
        "\n",
        "from scipy.integrate import odeint\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "camera = Camera(fig)\n",
        "\n",
        "for current_model in [-1] + [1, 34, 67, 100, 133, 166, 199]:\n",
        "    i = 4 #Use this simulation\n",
        "    if current_model > len(recorded_models):\n",
        "        continue\n",
        "\n",
        "    #Truth:\n",
        "    cutoff_time = 300\n",
        "    times = onp.array(s.times)[:cutoff_time]\n",
        "    x_times = onp.array(data[i, :cutoff_time])\n",
        "    masses = x_times[:, :, -1]\n",
        "    length_of_tail = 75\n",
        "\n",
        "    #Learned:\n",
        "    e = edge_index.cuda()\n",
        "    ogn.cpu()\n",
        "    if current_model > -1:\n",
        "        ogn.load_state_dict(recorded_models[current_model])\n",
        "    else:\n",
        "        # Random model!\n",
        "        ogn = OGN(n_f, msg_dim, dim, dt=0.1, hidden=hidden, edge_index=get_edge_index(n, sim), aggr=aggr).cuda()\n",
        "    ogn.cuda()\n",
        "    \n",
        "    def odefunc(y, t=None):\n",
        "        y = y.reshape(4, 6).astype(np.float32)\n",
        "        cur = Data(\n",
        "            x=torch.from_numpy(y).cuda(),\n",
        "            edge_index=e\n",
        "        )\n",
        "        dx = y[:, 2:4]\n",
        "        dv = ogn.just_derivative(cur).cpu().detach().numpy()\n",
        "        dother = np.zeros_like(dx)\n",
        "        return np.concatenate((dx, dv, dother), axis=1).ravel()\n",
        "\n",
        "    datai = odeint(odefunc, (onp.asarray(x_times[0]).ravel()), times).reshape(-1, 4, 6)\n",
        "    x_times2 = onp.array(datai)\n",
        "\n",
        "    d_idx = 10\n",
        "    for t_idx in range(d_idx, cutoff_time, d_idx):\n",
        "        start = max([0, t_idx-length_of_tail])\n",
        "        ctimes = times[start:t_idx]\n",
        "        cx_times = x_times[start:t_idx]\n",
        "        cx_times2 = x_times2[start:t_idx]\n",
        "        for j in range(n):\n",
        "            rgba = make_transparent_color(len(ctimes), j/n)\n",
        "            ax[0].scatter(cx_times[:, j, 0], cx_times[:, j, 1], color=rgba)\n",
        "            ax[1].scatter(cx_times2[:, j, 0], cx_times2[:, j, 1], color=rgba)\n",
        "            black_rgba = rgba\n",
        "            black_rgba[:, :3] = 0.75\n",
        "            ax[1].scatter(cx_times[:, j, 0], cx_times[:, j, 1], color=black_rgba, zorder=-1)\n",
        "\n",
        "        for k in range(2):\n",
        "            ax[k].set_xlim(-1, 3)\n",
        "            ax[k].set_ylim(-3, 1)\n",
        "        plt.tight_layout()\n",
        "        camera.snap()\n",
        "\n",
        "# camera.animate().save('multiple_animations_with_comparison.mp4')\n",
        "from IPython.display import HTML\n",
        "HTML(camera.animate().to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hLJGeV4IXtxc"
      },
      "source": [
        "# Symbolic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-LHESeG4XquY"
      },
      "source": [
        "Extract the force laws with the following procedure:\n",
        "- The data in `messages_over_time` correspond to inputs to, and features of, $\\phi^e$, recorded during each training epoch.\n",
        "- Select the last element of this list.\n",
        "- Find the most significant message feature. Each message feature corresponds to 'e1', 'e2', etc. Calculate the one with the largest standard deviation.\n",
        "\n",
        "Train [PySR](https://github.com/MilesCranmer/PySR) to fit this relationship.\n",
        "Thus, we have extracted a force law from the graph network without priors on the functional form.\n",
        "\n",
        "This is the same technique we used to extract the unknown dark matter overdensity equation from the Quijote simulations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hwgQjt-_ic5k"
      },
      "source": [
        "## Here's the best message, which we will study:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "G1rhnbq-XvG5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "### Selezione del messaggio migliore per effettuarne la regressione simbolica ###\n",
        "\n",
        "best_message = np.argmax([np.std(messages_over_time[-1]['e%d'%(i,)]) for i in range(100)])\n",
        "\n",
        "messages_over_time[-1][['e%d'%(best_message,), 'dx', 'dy', 'r', 'm1', 'm2']]\n",
        "\n",
        "### Esportazione in file .csv per comodità d'uso ###\n",
        "\n",
        "messages_over_time[-1][['e%d'%(best_message,), 'dx', 'dy', 'r', 'm1', 'm2']].to_csv(\"spring_best_messages.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lEKUuuYfTjP5",
        "IXpsg0LUb2sT",
        "9RTQhSJDbfLZ",
        "kdgdkJwFZl-V",
        "Zw9W7SB-Zl-i",
        "ARWJg6SbZl-n",
        "1PRxdcqSZl-z",
        "Fz05x2HLT5Kg",
        "kBtdfcvgZl_F",
        "gTHRRgTuZl_K",
        "g46wRgIWZl_S",
        "RszfVgfhZl_a",
        "WK5_u9F7UttT",
        "yyduWbndZl_e",
        "hSqOrC1WZl_l",
        "w12Qg4t_em8w",
        "Y4EzD79nU8DO",
        "Uxtqu4E4erXd"
      ],
      "name": "GN_Demo",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
